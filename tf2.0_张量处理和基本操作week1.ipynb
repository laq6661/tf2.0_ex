{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "print(tf.__version__)#输出版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-7d6310f0cc4a>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "print(tf.test.is_gpu_available())#gpu能用不\n",
    "print(tf.test.is_built_with_cuda())#cuda能用不"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2 3]\n",
      " [4 5 6]]\n",
      "tf.Tensor(\n",
      "[[1 2 3]\n",
      " [4 5 6]], shape=(2, 3), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "a  =  np.array([[1,2,3],[4,5,6]])\n",
    "print(a)\n",
    "my_tensor= tf.constant(a)#将里面的数据 可以是list或array转化为张量\n",
    "print(my_tensor)\n",
    "my_variable = tf.Variable(my_tensor) #tf默认占满显存的\n",
    "# print(my_variable)#转化为变量 ，变量可以完成大部分张量运算但不能reshape\n",
    "# #a = tf.Variable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 3)\n"
     ]
    }
   ],
   "source": [
    "print(my_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tf.float32"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_tensor2 = tf.zeros([2,2])#全0张量 老matlab了 注意写成列表形式\n",
    "print(my_tensor2.shape)\n",
    "my_tensor3 = tf.ones([2,3])\n",
    "tf.dtypes.DType(my_tensor3[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "几个基本的tf操作有相关的去找这些看能不能用就行了\n",
    "跟tf.keras同级别相关大类操作的综合的那个class\n",
    "tf.strings \n",
    "tf.as_string()把每个输入转化为字符串\n",
    "tf.string_to_number()\n",
    "tf.string_split()\n",
    "等等......\n",
    "tf.strings的其中一个重要的作用是可以使字符串成为TensorFlow的第一公民，可以直接加入到模型的输入中\n",
    "详细来说，我们之前在NLP中如果要将字符串进行计算，需要进行下面几步：\n",
    "\n",
    "首先需要将字符串分词，例如英文常见用空格、标点分词，中文使用分词器或者干脆按字分词\n",
    "其次需要计算一个词表，能将每个词对应到一个对应的数字上，一般还要加入一些例如[pad]，[unk]等特殊符号\n",
    "在训练前将训练集的所有字符串经过上面的结果，都转换为数字符号。或者使用generator等技术在训练中流式转换\n",
    "那么tf.strings的目的，就是我们为什么不能直接将字符串输入，避免上面的几步？这样做有几个好处：\n",
    "\n",
    "避免了很多多余的代码，比如额外的分词、计算词表等\n",
    "保证模型的统一性，例如模型本身就包含了分词和符号转换，就可以直接把模型打包、发布（例如用tensorflow hub），这样别人可以不加载或使用任何第三方代码和程序也能直接用你的模型了\n",
    "模型发布也可以直接用tensorflow serve等完成，避免第三方介入\n",
    "\n",
    "tf.debugging 调试用\n",
    "其中tf.Assert\n",
    "tf.debugging.assert_all_finite判断张量中不包含nan或inf\n",
    "\n",
    "tf.dtypes \n",
    "tf.as_type()类型转换 \n",
    "例如tf.complex()把两组实数对应组合（实部，虚部）组成复数\n",
    "tf.dtypes.DType()表示元素张量是什么类型\n",
    "...\n",
    "\n",
    "tf.math各种计算\n",
    "tf.math.abs\n",
    "tf.math.add\n",
    ".....\n",
    "\n",
    "tf.random各种随机\n",
    "\n",
    "tf.feture_column 将各种类型的原始数据转化为神经网络能处理的数值数据也能进行标准化离散化分箱等\n",
    "当然还有别的Class看就完了 下面看一下上面介绍的例子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.strings操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([b'h' b'e' b'l' b'l' b'o'], shape=(5,), dtype=string)\n",
      "tf.Tensor([b'hello' b'world'], shape=(2,), dtype=string)\n",
      "tf.Tensor([b'h' b'lll' b'qq'], shape=(3,), dtype=string)\n",
      "tf.Tensor([8 1], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "print(tf.strings.bytes_split('hello'))#按字符切割\n",
    "print(tf.strings.split('hello world'))#单词切割（默认空格）注意出来之后都是tensor格式哦\n",
    "print(tf.strings.split('h/lll/qq',sep='/'))#按sep切割\n",
    "print(tf.strings.to_hash_bucket(['hello','world'],num_buckets= 10))#hash编码把字符映射为id，bum_buckets hash有多少数字"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(5, 5), dtype=int32, numpy=\n",
       "array([[8, 8, 9, 9, 4],\n",
       "       [7, 2, 7, 1, 6],\n",
       "       [3, 6, 1, 8, 9],\n",
       "       [0, 5, 6, 3, 3],\n",
       "       [3, 0, 1, 9, 4]], dtype=int32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = tf.random.uniform(shape=(5,5),minval=0,maxval=10,dtype=tf.int32,seed=233)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = tf.random.uniform(shape=(2,2))\n",
    "tf.debugging.assert_equal(x=a.shape,y=(2,2))#判断两tensor是否相等相等就是true继续执行 不相等报错"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.debugging.assert_equal(x=[1,2,3],y=[1,2,4])#报错示范"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2 2]\n",
      " [3 5]]\n",
      "[[0 2]\n",
      " [3 3]]\n",
      "[[1 0]\n",
      " [0 4]]\n",
      "[[1 inf]\n",
      " [inf 4]]\n"
     ]
    }
   ],
   "source": [
    "a = tf.constant([[1,2],[3,4]])\n",
    "b = tf.constant([[1,0],[0,1]])\n",
    "\n",
    "tf.print(tf.math.add(a,b))#对应加法 tf.print直接printtensor里的数据其他信息shape dtype就不看了\n",
    "tf.print(tf.math.subtract(a,b))#-\n",
    "tf.print(tf.math.multiply(a,b))#对应数字相乘不是矩阵乘法\n",
    "tf.print(tf.math.divide(a,b))#除法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.constant([1.8,2.2],dtype=tf.float32)\n",
    "x1 = tf.dtypes.cast(x,tf.int32)#变成int类型\n",
    "x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常用层\n",
    "tf.nn：底层的函数库，其他各种库可以说都是基于这个底层库来进行扩展的。\n",
    "tf.keras.layers：如果说tf.nn是轮子，那么tf.keras.layers可以说是汽车。tf.keras.layers是基于tf.nn的高度封装。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
